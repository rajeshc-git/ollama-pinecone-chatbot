# ğŸ’¬ Hybrid RAG Chatbot with Ollama, Pinecone & FastAPI

This project demonstrates a fully local chatbot with Retrieval-Augmented Generation (RAG) capabilities using:

- ğŸ”— [Ollama](https://ollama.com/) for running LLMs like `llama3.2:1b` locally
- ğŸ§  [Pinecone](https://www.pinecone.io/) for vector search and context retrieval
- âš¡ FastAPI backend with real-time streaming
- ğŸ–¥ï¸ HTML frontend for user interaction

---

## ğŸ§  How It Works

1. User enters a message in the frontend.
2. Message is sent to the FastAPI `/chat` endpoint.
3. Pinecone returns relevant context.
4. Ollama is prompted with context + question.
5. Streaming response is shown to the user.

---

## ğŸ› ï¸ Setup Instructions

### Backend (FastAPI + Ollama)

```bash
pip install -r requirements.txt
uvicorn main:app --reload
