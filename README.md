# ğŸ’¬ Hybrid RAG Chatbot with Ollama, Pinecone & FastAPI

This project demonstrates a fully local chatbot with Retrieval-Augmented Generation (RAG) capabilities using:

- ğŸ”— [Ollama](https://ollama.com/) for running LLMs like `llama3.2:1b` locally
- ğŸ§  [Pinecone](https://www.pinecone.io/) for vector search and context retrieval
- âš¡ FastAPI backend with real-time streaming
- ğŸ–¥ï¸ HTML frontend for user interaction

---

## ğŸ§  How It Works

1. User enters a message in the frontend.
2. Message is sent to the FastAPI `/chat` endpoint.
3. Pinecone returns relevant context.
4. Ollama is prompted with context + question.
5. Streaming response is shown to the user.

---

## ğŸ› ï¸ Setup Instructions

### Backend (FastAPI + Ollama)

```bash
pip install -r requirements.txt
uvicorn main:app --reload

# ğŸ’¬ Hybrid RAG Chatbot with Ollama, Pinecone & FastAPI

A lightweight, privacy-respecting chatbot built using:

- ğŸ”— Ollama (local LLM: llama3.2:1b)
- ğŸ§  Pinecone vector DB for context retrieval
- âš¡ FastAPI backend
- ğŸ–¥ï¸ HTML frontend
- ğŸ’¬ Real-time streaming response

---

## ğŸ“ Folder Structure (suggested)

ollama-pinecone-chatbot/
â”œâ”€â”€ frontend/ # HTML, JS, CSS files
â”‚ â””â”€â”€ index.html
â”œâ”€â”€ main.py # FastAPI backend
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore



---

## ğŸ§  How It Works

1. User sends a message from the frontend.
2. Backend queries Pinecone for relevant data.
3. Prompt is created using the context + question.
4. Ollama responds with an answer (streamed).
5. Frontend shows the live response.

---

## ğŸ› ï¸ Setup Instructions

### Backend (FastAPI + Ollama)

```bash
pip install -r requirements.txt
uvicorn main:app --reload
Make sure Ollama is running:


ollama run llama3.2:1b
Edit main.py and set your Pinecone API Key and Assistant ID.

ğŸ“¦ Python Dependencies
Paste this in a file called requirements.txt:


fastapi
uvicorn
requests
sentence-transformers

ğŸ“„ License
MIT
